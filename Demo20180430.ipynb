{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Series and DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([['frank', 'M', 29], ['mary', 'F', 23], ['tom', 'M', 35], ['ted', 'M', 33], ['jean', 'F', 21], ['lisa', 'F', 20]])\n",
    "\n",
    "df.columns = ['name', 'gender', 'age']\n",
    "df\n",
    "\n",
    "\n",
    "s = pd.Series([11, 22, 33, 44, 55])\n",
    "s\n",
    "\n",
    "s.max()\n",
    "s.min()\n",
    "s.mean()\n",
    "s.describe()\n",
    "\n",
    "s[2]\n",
    "s[2:4]\n",
    "\n",
    "s.index = ['a', 'b', 'c', 'd', 'e']\n",
    "s\n",
    "\n",
    "s['c']\n",
    "\n",
    "age  = pd.Series([22,34,42])\n",
    "name = pd.Series(['mary', 'toby', 'sherry'])\n",
    "\n",
    "pd.DataFrame([name, age]).T\n",
    "\n",
    "\n",
    "df = pd.DataFrame([['frank', 'M', 29], ['mary', 'F', 23], ['tom', 'M', 35], ['ted', 'M', 33], ['jean', 'F', 21], ['lisa', 'F', 20]])\n",
    "\n",
    "df.columns = ['name', 'gender', 'age'] \n",
    "df\n",
    "\n",
    "df.describe()\n",
    "df.ix[1]\n",
    "\n",
    "df.ix[1:4]\n",
    "\n",
    "df[['name', 'age']]\n",
    "\n",
    "\n",
    "df['gender'] == 'M'\n",
    "df[df['gender'] == 'M']\n",
    "\n",
    "df[df['gender'] == 'M'].mean()\n",
    "df[df['gender'] == 'F'].mean()\n",
    "\n",
    "df.groupby('gender')['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SQLContext \n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "data_file = \"file:///tmp/ratings.txt\" \n",
    "raw_data = sc.textFile(data_file)\n",
    "raw_data.take(3)\n",
    "\n",
    "header = raw_data.first()\n",
    "header\n",
    "\n",
    "skip_data = raw_data.filter(lambda line: line != header)\n",
    "skip_data.take(3)\n",
    "\n",
    "csv_data = skip_data.map(lambda l: l.split('::'))\n",
    "csv_data.take(3)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "   userid = p[0],\n",
    "   itemid = p[1],\n",
    "   rating = int(p[2])\n",
    ")\n",
    ")\n",
    "row_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df = sqlContext.createDataFrame(row_data)\n",
    "#df.show(5)\n",
    "#df.take(5)\n",
    "# select itemid, rating from df where rating >= 4 limit 5\n",
    "df.filter('rating >= 4').select('itemid', 'rating').show(5)\n",
    "df.select('userid','rating').groupBy('userid').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df.registerTempTable(\"ratings\")\n",
    "#df.printSchema()\n",
    "ratings_data = sqlContext.sql(\"\"\"\n",
    "SELECT itemid,avg(rating) as avg_rating from ratings group by itemid order by avg(rating) desc limit 5\n",
    "\"\"\")\n",
    "ratings_data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將 Spark DataFrame 轉換為 rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_out  = ratings_data.rdd.map(lambda e: 'itemid: {}, rating: {}'.format(e.itemid, e.avg_rating))\n",
    "rating_out.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將 Spark DataFrame 轉換為 Pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = ratings_data.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找出最受歡迎的電影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 Movies DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "movie_raw    = sc.textFile('file:///tmp/u.item')\n",
    "movie_parsed = movie_raw.map(lambda l:l.split('|'))\n",
    "movie_row    = movie_parsed.map(lambda p : Row(\n",
    "        movieid   = p[0],\n",
    "        moviename = p[1]\n",
    "    )) \n",
    "movie_row.take(3)\n",
    "moviedf = sqlContext.createDataFrame(movie_row)\n",
    "moviedf.registerTempTable(\"movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 Ratings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rating_raw    = sc.textFile('file:///tmp/u.data')\n",
    "rating_parsed = rating_raw.map(lambda l: l.split())\n",
    "rating_row    = rating_parsed.map(lambda p : Row(\n",
    "        userid    = p[0],\n",
    "        movieid   = p[1],\n",
    "        rating    = int(p[2])\n",
    "    )) \n",
    "rating_row.take(3)\n",
    "ratingdf = sqlContext.createDataFrame(rating_row)\n",
    "ratingdf.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將兩個表格合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "moviedf.printSchema()\n",
    "ratingdf.printSchema()\n",
    "\n",
    "ratings_data = sqlContext.sql(\"\"\"\n",
    "    SELECT moviename,count(rating) as rating_cnt from ratings inner join movies on movies.movieid = ratings.movieid group by moviename order by rating_cnt DESC LIMIT 10\n",
    "\"\"\")\n",
    "#SELECT moviename,avg(rating) as avg_rating from ratings inner join movies on movies.movieid = ratings.movieid group by moviename order by avg_rating DESC LIMIT 10\n",
    "ratings_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "raw_data = sc.textFile(\"file:///tmp/churnTrain.csv\")\n",
    "raw_data.take(3)\n",
    "\n",
    "header = raw_data.first()\n",
    "header\n",
    "\n",
    "skip_data = raw_data.filter(lambda line : line != header)\n",
    "skip_data.take(3)\n",
    "\n",
    "splitlines = skip_data.map(lambda l: l.split(\",\"))\n",
    "splitlines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parseLine(col):\n",
    "    features = []\n",
    "    churn    = col[-1] \n",
    "    international  = 0 if col[4] == '\"no\"' else 1\n",
    "    voice          = 0 if col[5] == '\"no\"' else 1\n",
    "    label          = 0 if churn  == '\"no\"' else 1\n",
    "    features.append(international)\n",
    "    features.append(voice)\n",
    "    features += col[6:-1]\n",
    "    return LabeledPoint(label, Vectors.dense(features) )\n",
    "\n",
    "\n",
    "trainData = splitlines.map(parseLine)\n",
    "trainData.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立決策樹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "       \n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={},\n",
    "impurity='gini', maxDepth=5)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 印出決策樹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"Learned classification tree model:\") \n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "head = trainData.first() \n",
    "model.predict(head.features)\n",
    "\n",
    "predictions = model.predict(trainData.map(lambda p: p.features))\n",
    "#predictions.collect()\n",
    "filtered_labels_and_preds = labels_and_preds.filter(lambda v : v[0] == v[1]) \n",
    "test_accuracy = filtered_labels_and_preds.count() / float(trainData.count())\n",
    "test_accuracy\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics \n",
    "metrics = BinaryClassificationMetrics(labels_and_preds)\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR) \n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將資料區分為訓練與測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dataset = trainData.randomSplit([0.7,0.3])\n",
    "trainset = train_test_dataset[0]\n",
    "testset  = train_test_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根據訓練資料建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "       \n",
    "model = DecisionTree.trainClassifier(trainset, numClasses=2, categoricalFeaturesInfo={},\n",
    "impurity='gini', maxDepth=5)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根據測試資料評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "predictions = model.predict(testset.map(lambda p:\n",
    "p.features))\n",
    "labels_and_preds = testset.map(lambda p: p.label).zip(predictions)\n",
    "metrics = BinaryClassificationMetrics(labels_and_preds)\n",
    "\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR) \n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
